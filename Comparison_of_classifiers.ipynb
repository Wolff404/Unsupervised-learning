{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Classifiers for the Breast Cancer Prediction\n",
    "\n",
    "In this assignment we have been given the task to create a Decision Tree Classifier and a Naive Bayes Classifier, and compare the results of the two, from the given dataset.\n",
    "\n",
    "### *The dataset*\n",
    "We have been given the dataset Breast Cancer Prediction dataset. It contains data from cell nucleus in breast tissue, which is designed to assist diagnosing breastcancer. Here is an overview of the given dataset.\n",
    "\n",
    " - Consists of 569 instances/observations. \n",
    " - Has 30 numerical features. Excluding ID and diagnosis.\n",
    " - The features are real, meaning they will consist of floating point numbers.\n",
    " - Classification problem, where the goal is to classify cancer tumours, based on cell properties.\n",
    "\n",
    "\n",
    "#### *What is the purpose of this dataset?*\n",
    "The purpose of this dataset is as mentioned before, using data from cell nucleus in breast tissue to assist diagnosing breastcancer.\n",
    "\n",
    "#### *What are the features?*\n",
    "The features are numeric values that describe different properties of the tumor. Such as texture, radius, smoothness and etc.\n",
    "\n",
    "#### *What are the targets?*\n",
    "M and B, will act as our boolean outputs/targets. Where B is benign, which will act as our boolean 0. The M is malignant, the opposite of the benign, will act as our boolean 1.\n",
    "\n",
    "In other words, it can be simplified to as follows. B is for when the tumor is benign, and M is when tumor is malignant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T15:19:54.884002Z",
     "start_time": "2024-09-17T15:19:54.824053100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T15:19:54.913347100Z",
     "start_time": "2024-09-17T15:19:54.836391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  1      2      3       4       5        6        7       8        9       10  \\\n0  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n1  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n2  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n3  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n4  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n\n   ...     22     23      24      25      26      27      28      29      30  \\\n0  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n1  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860  0.2750   \n2  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430  0.3613   \n3  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n4  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625  0.2364   \n\n        31  \n0  0.11890  \n1  0.08902  \n2  0.08758  \n3  0.17300  \n4  0.07678  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('wdbc.data', sep=\",\", header=None)\n",
    "dataset.drop(columns=[0], inplace=True)\n",
    "dataset.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-prossesing of data\n",
    "Our pre-prossesing will consist of removing the ID, since it's not relevant for us. So we will use dataset.drop(columns=[0], inplace=True) to remove the ID coloumn.\n",
    "\n",
    "Also, since this is a .data file and not a .csv file, the first row is now a \"header\" for the other coloums. We can fix this by setting header=None.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "My approach to the splitting of the dataset will be a 60/20/20 split. This will also be a random sampling technique, meaning we will be splitting our data completely by random selecting them. It's also worth noting this needs to be without repetition.\n",
    "\n",
    "The option for choosing Stratified sampling was also considered. However, in our case, I will choose random selection for simplicity’s sake. It could be a possibility to use Stratified sampling if the dataset contained more \"boolean features\". Meaning the more clearly representative features that we could use to categorize more easily than floating numbers. In other words, it would be to organize by specific features to groups, before selecting. Which ensures the sample to be representative.\n",
    "\n",
    "\n",
    "- 60% for training\n",
    "  - This will ensure the model has enough data to learn from. In other words, it should be sufficient enough to capture the patterns and complexities within the data.\n",
    "\n",
    "- 20% for validation\n",
    "  -  This allow us to tune our hyperparamaters and make decisions according to our performance, without overfitting to the training data.\n",
    "\n",
    "- 20% for testing\n",
    "  - This will be used for having an unbiased evaluation of the models performance. This occurs after the training and the validation, to ensure its evaluating data it has not seen before.\n",
    "\n",
    "### *Comparing our split choice with the 80/10/10 split*\n",
    "\n",
    "When arriving at the question of choosing the correct split, it stood between two options. Either the 60/20/20 split or 80/10/10. So to elaborate why I didnt choose the 80/10/10, here is a few reasons.\n",
    "\n",
    "- The size of the dataset is\n",
    "- My interpretation of choosing the 80/10/10 split is for larger datasets.\n",
    "\n",
    "\n",
    "### *Defending the chosen 60/20/20 split*\n",
    "\n",
    "The reason why I think this is a good approach is that there will be sufficent data for the training to yield good results without overfitting. It has a balanced evaluation which provides flexibility and reliability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
